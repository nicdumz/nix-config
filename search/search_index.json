{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Over the 2024 end of year break, I completed a project to migrate my personal computing environments to a modern, declarative configuration model. I'm quite excited about where I've landed. I've created this mini-site to document the 'why' and 'how,' focusing on the gains in reliability, security, and velocity.</p> <p>The short version is that I've moved to a declarative Infrastructure as Code setup for all of my hosts' operating systems, configuration, and home directories. I've also pivoted to a different way of modeling and thinking about my machines, separating somewhat strictly configuration, from state, from data.</p> <p>That being said I don't believe in using \"tech trend\" words to describe setups, because these words tend to only mean something once you invest enough into the underlying trends. I'll try my best to describe what I did from a high-level point of view first, staying away from trends and tech-specific jargon.</p>"},{"location":"#the-journey","title":"The journey","text":"<p>How did I get there?</p>"},{"location":"#dotfiles-management","title":"Dotfiles management","text":"<p>Through my software engineering career, I've always versioned my personal configuration files, with the goal of being able to switch to a new machine and setup my developer environment with a single command, getting back to my usual levels of productivity in ~minutes. I don't want to be stressed about the idea of losing or breaking a laptop, or having a hard-drive failing on me.</p> <p>At work, I somewhat regularly grab a completely new VM and start afresh, to make sure I always version my entire configuration, and test my setup scripts regularly. This makes me rather content and feeling \"safe\" about my developer environment. I've witnessed, on the other hand, many colleagues dreading and pushing back their workstation or VM upgrades, or having to postpone project work until they fix a particular aspect of their developer workflow...</p> <p>This was in my opinion a small long-term investment which very quickly paid off. I spent perhaps a dozen days on this topic (cumulated), and this started yielding returns long ago.</p> <p>This is often referred to a \"dotfiles\" management, with https://dotfiles.github.io/ giving you a great primer. By now this concept is popular enough and I'd assume that the majority of professional software developers have comparable setups.</p>"},{"location":"#wishing-to-extend-this-to-the-os-layers","title":"Wishing to extend this to the OS layers","text":"<p>But, of course, I looked into generalizing this approach to other areas, e.g. how much can I fully automate and describe the final intended state of my machines and systems configurations.</p> <p>Unfortunately, \"Dotfiles\" narrowly focuses on configuring often already-installed tools. On my (now legacy) nicdumz/dotfiles setup, I did extend over time the basic functionality, and added a couple of scripts to install Debian packages automatically, to, again, be as close as possible to a \"single script to setup my environment\" workflow.</p> <p>However, this quickly gets complicated if you own machines with diverse setups (Mac laptop vs Debian workstation?), and in general doesn't take care of operating system level settings.</p> <p>Linux sysadmins sometimes use or tout etckeeper to \"version\" system configuration files, but this in my opinion falls short in several ways. The most obvious problem in my opinion is that this approach does not distinguish between system defaults -- which shipped with the Operation System or a particular package from that OS -- from your own customizations.</p> <p>And, of course, you can litter your <code>/etc/</code> configuration files with comments about your intent:</p> /etc/ssh/ssh_config<pre><code># BEGIN ndumazet 22/06/2022: make sure that chickens are allowed in\n# whilst wolves stay out.\nSomeSetting value\n# END ndumazet\n</code></pre> <p>which minimizes the pain during OS level software updates, but every time I looked in this area I never found fully satisfying solutions.</p>"},{"location":"#not-quite-orchestration","title":"Not quite: Orchestration","text":"<p>Docker Compose, Ansible, and similar tools would get close to what I seemed to need. I had experience with Docker Compose to maintain my media server, and it was really useful to setup N images and configure them in a versioned repository.</p> <p>While these tools manage services effectively, they don't typically manage the entire operating system with the same granular, reproducible control. This still left open the base OS level configuration up to the system administrator, meaning \"configuration drift\" was still a risk.</p>"},{"location":"#my-every-business-day-iac-at-work","title":"My every business day: IaC at work","text":"<p>At work, I configure production very much with an IaC approach.</p> <pre><code>graph LR\nA@{ shape: docs, label: \"Versioned configuration\"}\nC@{ shape: procs, label: \"Production processes\"}\nD@{ shape: cyl, label: \"Production databases\" }\nE@{ shape: cloud, label: \"Other production assets\" }\n  A --&gt; B([Deploy]);\n  B --&gt; C;\n  B --&gt; D;\n  B --&gt; E;</code></pre> <p>The internal Google configuration language is called <code>Prodspec</code> and the control plane is called <code>Annealing</code>.</p> <p>I'm very much used to only modifying the left-most box in this diagram: iterate on the configuration and wait for it to be deployed. Long gone are the times where I can modify directly the target systems on the right.</p>"},{"location":"#final-form-iac-for-my-os","title":"Final form: IaC for my OS","text":"<p>What I've really been after, all along, was something like this:</p> <pre><code>graph LR\nA@{ shape: docs, label: \"Versioned configuration\"}\nC@{ label: \"Router\" }\nD@{ label: \"Laptop\" }\nE@{ label: \"Workstation\" }\n  A --&gt; B([Apply]);\n  B --&gt; C;\n  B --&gt; D;\n  B --&gt; E;</code></pre> <p>Versioned configuration, leading to reproducible system-wide configurations.</p> <p>Taken to an extreme: I can take my Laptop, completely wipe its state or replace its disks with new disks, deploy again the OS configuration to this fresh hardware, and I will obtain an identical system. I don't have to worry about the hard drive failing or about the hardware melting, because my systems are configured in such a way that I can assume these things will fail or will need re-imaging.</p> <p>How to get into this shape has been interesting, and the next pages on this mini website will expand on how we got there.</p>"},{"location":"conclusion/","title":"Conclusion","text":"<p>Ultimately, the combination of Nix and NixOS is more than just a configuration tool; it's a paradigm shift towards an immutable, declarative infrastructure model.</p> <p>This model is quite similar to how I'm used to thinking of production at work, and I'm surprised it took me so long to figure out I could do such a thing at home.</p>"},{"location":"conclusion/#debian-pales-in-comparison","title":"Debian pales in comparison","text":"<p>I used to run all of my home machines (laptop, workstations, router) on Debian. I unfortunately regularly had events where on a day where you really need your machine, Debian boots up with a broken graphics driver and/or no audio after a magical \"stable\" critical update, to great frustration, and without any easy way to diagnose quickly what happened.</p>"},{"location":"conclusion/#i-wont-be-going-back","title":"I won't be going back","text":"<p>A year into my experiment, I am now very sure.</p> <p>Mark my words: there is no way I will be running an Operating System that's not declarative. Perhaps I'll be switching to a future OS generation, better than NixOS, but a declarative, intent based configuration model seems to be the one and only way.</p>"},{"location":"details/","title":"Details","text":""},{"location":"details/#critical-updates","title":"Critical updates","text":"<p>NixOS provides us with deterministic, reproducible \"generations\" for your operating system. At the same time we did say that we wanted to pick up regular upstream changes, critical security and other fixes. This seems at odds with the idea of a \"static\" configuration repository describing an unchanging final intent.</p> <p>Upstream repositories (<code>nixpkgs</code>) release regular changes on a stable branch, we \"only\" have to pick up those new changes.</p> <p>Because everything is versioned, this requires automated repository changes, similarly to <code>Dependabot</code> and <code>Renovate</code> bots picking up the latest versions of npm packages and updating <code>package.json</code> in repositories.</p> <p>I have a small GitHub action taking care of this task for me. Note that unlike the npm ecosystem picking up versions from head (and what this means for security and reliability), I only track the stable version of <code>nixpkgs</code>/<code>NixOS</code>, which mitigates risks somewhat.</p>"},{"location":"details/#continuous-integration-and-caching","title":"Continuous Integration and Caching","text":"<p>For each Pull Request (including the aforementioned automated updates) I rebuild entirely all of my systems (!) and validate that no major assertions trigger during the build process.</p> <p>This may sound like a very expensive step to do as part of CI, but because of the distributed caching, this is actually rather affordable.</p> <p>As a bonus effect: a change going through CI will pre-warm the distributed caches, guaranteeing that subsequent updates on my machines will be able to fetch already built artefacts already.</p> <p>Nix package maintainers will include some verifications over configuration, giving builds a chance to fail before booting the system on a broken generation. I wouldn't say that this is foolproof though, perhaps 1 upgrade in 10 may result in broken generation. A broken generation on NixOS is however a non-event as you can simply select the previous generation in the <code>systemd-boot</code> menu.</p>"},{"location":"details/#major-os-upgrades","title":"Major OS upgrades","text":"<p>Twice a year, NixOS releases new versions. Upgrading is very similar to the automated upgrades I mentioned above (tracking a new upstream branch), and are just as uneventful, so far.</p> <p>Worst case, I can roll back (in my boot menu) to the previous generation if I fail to boot into the latest version.</p>"},{"location":"details/#nixos-for-my-router","title":"NixOS for my router","text":"<p>I own a Linux box which does some 10G routing, and acts as a media server, all for the benefit of the household.</p> <p>I cannot afford to break this box for long. That being said, once again, most of the time even a scary outage can just be fixed by a power reset on the box, booting back into the previous NixOS generation.</p>"},{"location":"details/#secrets-on-the-open-internet","title":"Secrets on the open Internet","text":"<p>I had to check into my repository a few secrets, for instance generated SSH keys, or user passwords, in order to have a deterministic setup.</p> <p>There are easy ways to do this safely nowadays when you own enough FIDO2 keys. Simply put, you can encrypt your secrets for your FIDO2 key's identities. When provisioning a new host, you can rewrap those secrets (one security key touch!) to re-encrypt them for the target host.</p>"},{"location":"model/","title":"Model","text":"<p>I personally find that my personal systems configurations have been difficult to model and maintain in part because I didn't spend time separating and defining what belongs where, or being as rigorous as what I would have to be in a professional / production environment.</p> <p>For once, I will formalize a model for system state. I claim that effective system maintenance requires strictly separating concerns into 6 categories:</p> <p>Configuration, Cache, Data, Provisioning state, Telemetry, and Ephemeral. You should always be aware of what category you're working with, and act accordingly -- it'll make your life a lot easier in the long run.</p> <p>Let's get into it.</p>"},{"location":"model/#configuration","title":"Configuration","text":"<p> Versioned.</p> <p>The obvious one in an IaC setup: your intent configuration is where you describe the final state of your systems. It should be versioned. In my case this is nicdumz/nix-config.</p>"},{"location":"model/#cache","title":"Cache","text":"<p> Persisted locally, but OK to lose and rebuild.</p> <p>In my mental model, \"Cache\" is for artefacts and data that can be recomputed or downloaded again. The only cost to losing \"cache\" is having to spend compute or network resources to rebuild it again.</p> <p>Examples:</p> <ul> <li>If you use a browser sync feature, your browsing history is only \"cache\": you only have to sign   into your sync'ed profile to retrieve locally all of your browser's state.</li> <li>Any Git repo can be fetched again from its authoritative source.</li> <li>Any large ISO can be downloaded/torrented again.</li> <li>Binary packages, or outputs of reproducible compilations or transformations in general, can be   modeled as Cache: one \"only\" has to recompile them or to re-execute the transformation to rebuild   state.</li> </ul>"},{"location":"model/#data","title":"(Original) Data","text":"<p> Back it up or you'll lose it.</p> <p>Your family photos? The output of a very expensive to train ML model? Anytime you are the author of data, you are responsible for adequately organizing backups, from the source of the data, to reasonable redundant replicas, depending on the importance of this data.</p> <p>If you don't back this up or don't test your recovery story? You lose this information.</p> <p>In modern systems with many \"Cloud Sync\" options, you have surprisingly little original data with your systems as primary hosts. Aside: it might be worth questioning if you really need or want to be the main stakeholder for this data or if a service may do this better for you.</p> <p>Note: in the case of Cloud backed-up data, I consider the local copies of this data \"cache\" and not \"data\".</p>"},{"location":"model/#provisioning-state","title":"Provisioning state","text":"<p> Randomized at install time, regenerating it must be fine.</p> <p>When installing a new machine, some aspects will be randomly generated or specific to the particular deployment. As an example, you will most likely generate a local SSH key for the host, or disk partitioning will assign distinct UUIDs to disk partitions.</p> <p>In general these aspects are random, generated during the first installation of your machine, and must not change the overall behavior of your system. This means that it's fine to have different provisioning state across installs, and to lose this state on re-provisioning.</p> <p>Some nuances:</p> <ul> <li>The hostname of a machine, to me, is not part of provisioning state as it can be determined in   advance, and should be part of Configuration.</li> <li>While it may be needed to generate a private SSH key during machine deployment/provisioning, that   SSH key is not what I want to be referring to in my Configuration, as it will change across   machine installs. Instead, I version (encrypted) SSH private/public keypairs for my hosts in my   Configuration, and deploy them to my hosts after initial provisioning.</li> </ul>"},{"location":"model/#telemetry","title":"Telemetry, Observability","text":"<p> Observability, may or may not persist across reboots or reinstalls.</p> <p>System logs, console outputs. All of those help me diagnose what happened to a system. Ideally I should collect this data, aggregate it, and process it outside of the originating machines (with Vector or whichever Observability suite you like). In an idealized system I should even use this to build monitoring and alerting.</p> <p>On the other hand, losing this state \"only\" removes understanding of my system. Depending on tradeoffs, I sometimes keep this data local to machines, do not necessarily back it up, and sometimes this data does not even survive a reboot.</p>"},{"location":"model/#ephemeral","title":"Default, everything else: Ephemeral","text":"<p> Wiped on reboots.</p> <p>The Beyonce rule applies: \"if you liked it you should have versioned it\" (or backed it up).</p> <p>Erase your darlings and tmpfs as root were strong motivators here: there should be no in-between \"oh what is this <code>/home/foo/important.sh</code> file doing here\" moments when maintaining my systems. All of the changes should be done first and foremost in the configuration plane. Direct changes to machines will be ephemeral and lost on reboots, by design: don't do it, you'll only regret it.</p> <p>Similarly, if a process dumps data in a directory and expects it back on reboots? Too bad. This is a sign of misconfiguration / lacking an abstraction, and will break by design, until I fix it.</p>"},{"location":"requirements/","title":"Requirements","text":"<p>A brief list of requirements so we understand what the solution will look like:</p> <ul> <li> <p>Configuration MUST be versioned publicly.</p> <ul> <li>Versioned for all the goodness that comes with it (tracking why a   particular changes landed, when, in which context.)</li> <li>I want to be part of an open ecosystem. I want to contribute to the set    of configurations that Internet strangers can look up on GitHub and learn    from. (<code>\"path:*.nix something\"</code> is a regular GitHub search query of mine    when I develop on Nix / NixOS).</li> </ul> </li> <li> <p>System deployments or new machine installs MUST be   reproducible and deterministic.</p> <ul> <li>Importantly, if I deploy my system at a given Git revision, I get exactly the same output everytime. This allows for trivial pain-free rollbacks.</li> </ul> </li> <li> <p>Updates (security updates, OS updates, individual software version updates) MUST not break my   systems.</p> <ul> <li>Critical security updates MUST apply at least weekly.</li> <li>If a new version of a dependency becomes incompatible with my configuration, the <code>deploy</code> step SHOULD fail before changing my systems. When this is not feasible, rollbacks to an earlier Git revision MUST be a viable recovery path.</li> </ul> </li> </ul> <p>Non-functional:</p> <ul> <li>It SHOULD be easy for me to adhere to my philosophy and modeling principles,   separating data/state into one of my 6 pre-defined categories.</li> </ul>"},{"location":"solution/","title":"Solution","text":"<p>I'm using NixOS on all of my machines since end of 2024. An explanation of the involved pieces and how they all fit together.</p> <p>I've asked AI to write a teaser on why NixOS is cool, and I believe it did a pretty good job:</p> <p>What if you could try a massive system update, and if it broke anything\u2014your Wi-Fi, your monitor, a key app\u2014you could roll back your entire operating system to its previous, perfect state with a single reboot?</p> <p>That's the magic of NixOS.</p> <p>It treats your whole OS as a single \"package.\" This means:</p> <ul> <li> <p>Atomic Updates &amp; Rollbacks: Every upgrade creates a new \"generation\" of your system. If you don't   like it, your old one is still there, ready to be booted.</p> </li> <li> <p>No More \"Configuration Drift\": Your system is built only from your versioned config files. You   can't just temporarily hack a file in <code>/etc/</code> and forget about it. This ends the \"what did I   change to make this work?\" nightmare. Your versioned configuration is your system's single source   of truth.</p> </li> </ul>"},{"location":"solution/#nix","title":"Configuration language: Nix expression language","text":"<p>I believe that most configuration languages are created equal. Some syntax to produce key-value pairs, more or less. Syntactic sugar to avoid repetitions, but that's it.</p> <p>There's a 101 blog entry here, but that part in my opinion is quite boring.</p> <p>Nonetheless, all of the configurations are written in a collection of versioned <code>.nix</code> files, and that are various ways to organize your configuration in a modular fashion.</p> <p>Some examples from my configuration:</p> <ul> <li>How I configure VSCode   (only enabled on headful machines).</li> <li>How I configure Grafana   (only enabled on the router machine).</li> </ul>"},{"location":"solution/#nix-packages","title":"Nix packages","text":"<p>Using the Nix language, one can define \"derivations\", which are recipes for reproducible transformations. Each derivation describes \"build steps\" which takes a set of inputs and transforms them into outputs.</p> <p>Derivations are then used to build \"packages\", very similarly to software packages in a regular distribution, e.g. Debian.</p> <p>I think it's easier to look at an existing package to understand what happens. This package would fetch the source, via Github, of the <code>kakoune</code> editor, and <code>make; make install</code> for that package, producing an expected <code>kakoune</code> binary.</p> <p>I never have to write my own packages. The NixOS community maintains a central repository of packages at NixOS/nixpkgs. All put together, this makes a kind of \"Source distribution\", not too different from Gentoo where users have to recompile binaries on each update.</p> <p>Unlike Gentoo however, you rarely have to recompile packages, because the language around derivations forces output to be reproducible and deterministic:</p>"},{"location":"solution/#package-outputs","title":"Package outputs","text":"<p>All of my systems have a <code>/nix/store</code> folder, the \"Nix Store\":</p> Inside the Nix Store<pre><code>$ ls /nix/store/ | head\n0a1yz9lgzly1qdj2464gr1lmz2zpnxkl-libtool-2.4.7.drv\n0a2kbdrcsnmll5jndv98g63y83jzwhzi-gvfs-1.57.2\n0a2yia3avaw4n7sq9blfhjkw9bwaz845-umockdev-0.19.1.drv\n0a3facj8mq31kmazfh1ys00vwsqmwk7a-mdbook-linkcheck-0.7.7-vendor.tar.gz.drv\n0a4di192p2vbkvvq1skin6bx211vidrx-libXfont-1.5.4.drv\n0a4wbzik10grldjx3hcadzg337anzk5b-home-manager-path\n0a5m3qldpclgpbak4mkwlv5182sd1ax7-skylighting-core-0.14.5.drv\n0a7ddzdi7l58ay9vix4xzmvslnp31my9-Module-Runtime-0.016.tar.gz.drv\n0a7mfnca025rzk58ws0n7q47qpvjpcy3-libdvdread-6.1.3.drv\n</code></pre> <p>Those are outputs of derivations, using content-based path prefixes. The output of a derivation can be predicted from a nix package definition, meaning that:</p> <ul> <li>Users needing to depend on a package's output can simply link to the expected nix store path   output.</li> <li>Local caching: When deploying a new version of the system, if the derivation output already   exists in the Nix Store, then there is no need to recreate it (\"re-compile it\").</li> <li>Distributed remote caching: Users of Nix can share the output of computations on a networked   cache (https://cache.nixos.org/)</li> </ul> <p>The Nix store is essentially immutable -- paths are read-only to all users, and a path is only garbage collected if nothing refers to it.</p>"},{"location":"solution/#package-manager","title":"Package manager","text":"<p>Let's start simple, imagine that you want to use the output of Nix packages in your $PATH. We know that the outputs are in the Nix store and can be content-addressed. You could then... simply add the raw paths to your $PATH. In fact, the Nix package manager does something similar. It sets up symlinks against Store paths in a \"profile\":</p> User (PATH) profiles<pre><code>$ ls -la /etc/profiles/per-user/ndumazet/bin/\n...\nlrwxrwxrwx - root  1 Jan  1970 git -&gt; /nix/store/7kh7s643w6brdzmbk28pzk5z13zgcbax-home-manager-path/bin/git\n...\nlrwxrwxrwx - root  1 Jan  1970 nvim -&gt; /nix/store/7kh7s643w6brdzmbk28pzk5z13zgcbax-home-manager-path/bin/nvim\n# There are a few of those\n$ ls -la /etc/profiles/per-user/ndumazet/bin/ | wc -l\n45\n</code></pre> <p>And <code>/etc/profiles/per-user/ndumazet/bin</code> is in my <code>$PATH</code>.</p> <p>I've simplified a lot, but this is essentially how the NixOS ecosystem will be managing resources and binaries.</p> <p>Under the hood, you can imagine that all binaries are in fact statically linked or point to library paths which refer to Nix store paths. A corollary is that it becomes trivial to install several versions of a binary which all require distinct versions of a library, without risking the integrity of your system. This completely solves the \"dependency hell\" type of problems. You don't need tools like <code>virtualenv</code> just to manage different project requirements; Nix handles it at the operating system level.</p>"},{"location":"solution/#nixos","title":"NixOS","text":"<p><code>NixOS</code> is an operating system building on all of the above.</p> <p>My system can currently boot 3 generations:</p> System profiles<pre><code>$ ls -la /nix/var/nix/profiles\nlrwxrwxrwx - root  3 Jan 21:25 default -&gt; /nix/var/nix/profiles/per-user/root/profile\ndrwxr-xr-x - root  3 Jan 17:43 per-user\nlrwxrwxrwx - root 25 Oct 06:57 system -&gt; system-234-link\nlrwxrwxrwx - root  6 Sep 08:28 system-223-link -&gt; /nix/store/m3x7xfxrydw8kamk31ky3vgs567daibx-nixos-system-bistannix-25.05.20250904.fe83bbd\nlrwxrwxrwx - root 19 Oct 15:10 system-233-link -&gt; /nix/store/cihzg1vqz4m0g16w8lkglvbb9vjvm2i3-nixos-system-bistannix-25.05.20251016.98ff3f9-flake.20251019.e7299b0\nlrwxrwxrwx - root 25 Oct 06:57 system-234-link -&gt; /nix/store/0syb258lyj0kfamd7i7kwi0r98b99vrj-nixos-system-bistannix-25.05.20251021.481cf55-flake.20251023.0febaec\n</code></pre> <p>Each generation (<code>system-${gen}-link</code>) points to a Store path, which is an immutable directory to a top level \"system package\".</p> <p>What's inside that Store directory path? A root-like filesystem, with more symlinks into the nix store:</p> Inside a profile<pre><code>$ ls -la /nix/var/nix/profiles/system-233-link/\n.r-xr-xr-x 8.3k root  1 Jan  1970 activate\nlrwxrwxrwx    - root  1 Jan  1970 append-initrd-secrets -&gt; /nix/store/854r1y7ds8gpb590ykp80pp8abxvh8rz-append-initrd-secrets/bin/append-initrd-secrets\ndr-xr-xr-x    - root  1 Jan  1970 bin\n.r--r--r--  789 root  1 Jan  1970 boot.json\n.r-xr-xr-x 3.4k root  1 Jan  1970 dry-activate\nlrwxrwxrwx    - root  1 Jan  1970 etc -&gt; /nix/store/x625sh59bplvfq71rmh93i5mi762fc9a-etc/etc\n.r--r--r--    0 root  1 Jan  1970 extra-dependencies\nlrwxrwxrwx    - root  1 Jan  1970 firmware -&gt; /nix/store/ikkg1182hfwpghgh3afp16nmw3q3zclr-firmware/lib/firmware\n.r-xr-xr-x 172k root  1 Jan  1970 init\n.r--r--r--    9 root  1 Jan  1970 init-interface-version\nlrwxrwxrwx    - root  1 Jan  1970 initrd -&gt; /nix/store/04abzzbp0rymax41i7qgwrrlqvwh8ajl-initrd-linux-6.12.53/initrd\nlrwxrwxrwx    - root  1 Jan  1970 kernel -&gt; /nix/store/z05bjh6ihlb04v2l1id59baxk1qdxpdz-linux-6.12.53/bzImage\nlrwxrwxrwx    - root  1 Jan  1970 kernel-modules -&gt; /nix/store/kmh5d3qv20n5l4y96q63agjf552a5xmi-linux-6.12.53-modules\n.r--r--r--   43 root  1 Jan  1970 kernel-params\n.r--r--r--   45 root  1 Jan  1970 nixos-version\n.r-xr-xr-x 4.4k root  1 Jan  1970 prepare-root\ndr-xr-xr-x    - root  1 Jan  1970 specialisation\nlrwxrwxrwx    - root  1 Jan  1970 sw -&gt; /nix/store/4vmgbxnmsd7wi550f3va9m48shadqrwv-system-path\n.r--r--r--   12 root  1 Jan  1970 system\nlrwxrwxrwx    - root  1 Jan  1970 systemd -&gt; /nix/store/d84f8nm2na5cr53m4jk0qk2mj7lgr9fx-systemd-257.9\n</code></pre> <p>You can see the usual initramfs, kernel, which you would expect on other distributions.</p> <p>Put together, this means that each <code>/nix/var/nix/profiles/system-${gen}-link/</code> path is an immutable version of linux distribution, at generation <code>${gen}</code>.</p> <ul> <li>When booting up, systemd-boot presents those 3 generations in a boot menu, and the boot sequence   will deploy the selected, particular system's generation.</li> <li>When upgrading your system, you will first build all your dependencies, create the root   filesystem, and the upgrade process will atomically swap to another generation, by overwriting a   couple symlinks.</li> </ul>"},{"location":"solution/#is-mostly-a-tmpfs","title":"<code>/</code> is mostly a tmpfs","text":"<p>Inspired by Erase your darlings and tmpfs as root, <code>/</code> root filesystem is not persistent.</p> <p>There are particular mountpoints, e.g. <code>/nix/</code> mentioned above, which survive reboots, but the vast majority of the filesystem is ephemeral or a long tree of links:</p> Where is sshd_config<pre><code>$ ls -la /etc/ssh/sshd_config\nlrwxrwxrwx - root 27 Oct 17:41 /etc/ssh/sshd_config -&gt; /etc/static/ssh/sshd_config\n$ ls -la /etc/static/ssh/sshd_config\nlrwxrwxrwx - root  1 Jan  1970 /etc/static/ssh/sshd_config -&gt; /nix/store/lvb4syxyhzdjzqvxklb87kvzzygpfiny-sshd.conf-final\n$ ls -la /nix/store/lvb4syxyhzdjzqvxklb87kvzzygpfiny-sshd.conf-final\n.r--r--r-- 975 root  1 Jan  1970 /nix/store/lvb4syxyhzdjzqvxklb87kvzzygpfiny-sshd.conf-final\n</code></pre> <p>This solution elegantly prevents me from hacking configuration files directly. For example, <code>/etc/ssh/sshd_config</code> is just a symlink pointing (indirectly) to a read-only file in <code>/nix/store/</code>. If I need to change a setting, I must go to the intent source, modify the versioned .nix file, and rebuild my system.</p>"}]}